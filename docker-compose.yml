version: "3.9"

services:
  backend:
    build:
      context: ./backend
    image: ai-article-summarizer-backend
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3}
    ports:
      - "8080:8080"

  frontend:
    build:
      context: ./frontend
    image: ai-article-summarizer-frontend
    ports:
      - "5173:80"
    depends_on:
      - backend

